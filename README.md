# ğŸ§  IT Audit using Generative AI (Mistral via Ollama)

## ğŸš€ Overview

This project offers a minimal yet functional tool for analyzing IT configurations (in JSON format) using a local, open-source LLM model, via the Ollama interface and the Mistral model.

The tool is based on a simple architecture:

- **Backend:** FastAPI for file processing  
- **Frontend:** React interface for uploading and displaying results  
- **LLM:** Mistral model launched locally with Ollama  

## ğŸ”§ Prerequisites

- Docker Desktop  
- Ollama with the Mistral model:  
  ```
  ollama run mistral
  ```  
- Node.js (for the frontend)  

## âš™ï¸ Installation & Launch

### ğŸ–¥ï¸ Backend (FastAPI via Docker)

```
cd backend
docker build -t audit-backend .
docker run -p 8000:8000 audit-backend
```

### ğŸŒ Frontend (React in development mode)

```
cd frontend
npm install
npm run dev
```

The interface is then accessible at: [http://localhost:5173](http://localhost:5173)

## ğŸ§ª Usage

- Select a configuration file in `.json` format  
- Click **Analyze** to send it to the backend API  
- Read the summary generated by the LLM in the lower section  

## ğŸ“„ License

AGPL v3 â€” Free software with strong reciprocity. Any reuse or public modification must also be published under the AGPL license.  

## ğŸ“¬ Contact

For any questions, collaboration, or user feedback:  
ğŸ“§ [humanologue@gmail.com](mailto:humanologue@gmail.com)  

## ğŸ“Œ Notes

- This repository does not contain any sensitive deployment code.  
- The actual infrastructure or real IT data is neither included nor described here.  
- This is a freely reusable and extensible MVP.
```
